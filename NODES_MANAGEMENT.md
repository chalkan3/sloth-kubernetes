# ğŸ“Š Node Management - Guia Completo

## VisÃ£o Geral

O comando `kubernetes-create nodes` permite gerenciar completamente os nodes do seu cluster Kubernetes, incluindo listar, adicionar, remover, acessar via SSH e fazer upgrade.

---

## ğŸ¯ Comandos DisponÃ­veis

```bash
kubernetes-create nodes list              # Listar todos os nodes
kubernetes-create nodes add --count 2     # Adicionar workers
kubernetes-create nodes remove <name>     # Remover node
kubernetes-create nodes ssh <name>        # SSH no node
kubernetes-create nodes upgrade           # Upgrade Kubernetes
```

---

## ğŸ“‹ 1. Listar Nodes

### Uso BÃ¡sico

```bash
# Listar todos os nodes do cluster
kubernetes-create nodes list
```

### Output Exemplo

```
ğŸ“Š Cluster Nodes

Nodes:

NAME             ROLE     PROVIDER       REGION    STATUS      IP ADDRESS
----             ----     --------       ------    ------      ----------
master-1         master   DigitalOcean   nyc3      âœ… Ready    167.71.1.1
master-2         master   Linode         us-east   âœ… Ready    172.105.1.1
master-3         master   Linode         us-east   âœ… Ready    172.105.1.2
worker-1         worker   DigitalOcean   nyc3      âœ… Ready    167.71.1.2
worker-2         worker   DigitalOcean   nyc3      âœ… Ready    167.71.1.3
worker-3         worker   Linode         us-east   âœ… Ready    172.105.1.3

ğŸ“Š Summary:

  â€¢ Total Nodes: 6
  â€¢ Masters: 3 (HA)
  â€¢ Workers: 3

  â€¢ DigitalOcean: 3 nodes
  â€¢ Linode: 3 nodes

  âœ… All nodes healthy
```

### Com Stack EspecÃ­fico

```bash
# Listar nodes de um stack especÃ­fico
kubernetes-create nodes list --stack production
kubernetes-create nodes list --stack staging
```

### InformaÃ§Ãµes Mostradas

- âœ… **Nome do node**
- âœ… **Role** (master/worker)
- âœ… **Provider** (DigitalOcean/Linode)
- âœ… **RegiÃ£o**
- âœ… **Status** (Ready/NotReady)
- âœ… **EndereÃ§o IP pÃºblico**
- âœ… **Resumo do cluster**

---

## â• 2. Adicionar Nodes

### Adicionar Workers (Simples)

```bash
# Adicionar 2 workers com configuraÃ§Ã£o padrÃ£o
kubernetes-create nodes add --count 2
```

### Adicionar com ConfiguraÃ§Ãµes EspecÃ­ficas

```bash
# Adicionar 3 workers com size especÃ­fico
kubernetes-create nodes add \
  --count 3 \
  --size s-4vcpu-8gb \
  --role worker

# Adicionar em provider especÃ­fico
kubernetes-create nodes add \
  --count 2 \
  --provider linode \
  --region us-east

# Adicionar a um pool especÃ­fico
kubernetes-create nodes add \
  --count 1 \
  --pool do-workers
```

### Flags DisponÃ­veis

| Flag | DescriÃ§Ã£o | PadrÃ£o |
|------|-----------|--------|
| `--count` | NÃºmero de nodes | 1 |
| `--role` | Role (master/worker) | worker |
| `--size` | Tamanho do node | Do config |
| `--provider` | Provider (digitalocean/linode) | Do config |
| `--region` | RegiÃ£o | Do config |
| `--pool` | Nome do pool | Auto-detect |

### Workflow Recomendado

```bash
# 1. Ver configuraÃ§Ã£o atual
kubernetes-create nodes list

# 2. Editar cluster config para aumentar count
vim cluster.yaml
# nodePools:
#   - name: workers
#     count: 5  # Era 3, agora 5

# 3. Preview das mudanÃ§as
kubernetes-create deploy --config cluster.yaml --dry-run

# 4. Aplicar as mudanÃ§as
kubernetes-create deploy --config cluster.yaml

# 5. Verificar novos nodes
kubernetes-create nodes list
```

### âš ï¸ Importante

- **Masters**: Sempre mantenha nÃºmero Ã­mpar (1, 3, 5) para HA
- **Workers**: Pode adicionar qualquer quantidade
- **Custos**: Cada node tem custo mensal no provider
- **Tempo**: Provisionamento leva ~5-10 minutos

---

## â– 3. Remover Nodes

### Remover um Worker

```bash
# Remover worker especÃ­fico
kubernetes-create nodes remove worker-3
```

### Remover com Force (sem confirmaÃ§Ã£o)

```bash
# Remover sem pedir confirmaÃ§Ã£o
kubernetes-create nodes remove worker-3 --force
```

### Processo de RemoÃ§Ã£o

O comando executa:

1. **Drain** - Move todos os pods para outros nodes
2. **Delete** - Remove o node do Kubernetes
3. **Destroy** - Destroi o recurso no cloud provider

### Output Exemplo

```
â– Removing Node: worker-3

âš ï¸  WARNING: This will:
  1. Drain all pods from the node
  2. Remove the node from Kubernetes
  3. Destroy the cloud resource

Are you sure you want to remove worker-3? (y/N): y

â³ Draining node...
âœ… Node drained successfully

â³ Deleting from Kubernetes...
âœ… Node deleted from cluster

â³ Destroying cloud resource...
âœ… Resource destroyed

âœ… Node worker-3 removed successfully!
```

### Workflow Recomendado

```bash
# 1. Ver nodes atuais
kubernetes-create nodes list

# 2. Editar config para reduzir count
vim cluster.yaml
# nodePools:
#   - name: workers
#     count: 2  # Era 3, agora 2

# 3. Preview
kubernetes-create deploy --config cluster.yaml --dry-run

# 4. Aplicar
kubernetes-create deploy --config cluster.yaml

# 5. Verificar
kubernetes-create nodes list
```

### âš ï¸  Avisos

- **Dados**: Certifique-se que nÃ£o hÃ¡ dados importantes no node
- **Pods**: Pods com PersistentVolumes podem ter problemas
- **Masters**: NUNCA remova masters sem backup
- **HA**: Mantenha pelo menos 3 masters para HA

---

## ğŸ” 4. SSH nos Nodes

### SSH em um Node EspecÃ­fico

```bash
# SSH no master-1
kubernetes-create nodes ssh master-1

# SSH no worker-2
kubernetes-create nodes ssh worker-2
```

### O Que Acontece

1. CLI busca o IP do node
2. Recupera a chave SSH do stack
3. Abre sessÃ£o SSH interativa

### Output Exemplo

```
ğŸ” SSH to Node: master-1

â³ Fetching node information...
âœ… Node found: 167.71.1.1

ğŸ”‘ Using SSH key from cluster deployment

Connecting to root@167.71.1.1...

root@master-1:~#
```

### Comandos Ãšteis no Node

```bash
# Ver status do RKE2
systemctl status rke2-server  # Master
systemctl status rke2-agent   # Worker

# Ver logs do RKE2
journalctl -u rke2-server -f
journalctl -u rke2-agent -f

# Ver containers rodando
crictl ps

# Ver uso de recursos
top
htop
df -h

# Ver configuraÃ§Ã£o RKE2
cat /etc/rancher/rke2/config.yaml

# Ver kubeconfig (apenas master)
cat /etc/rancher/rke2/rke2.yaml
```

### SSH Manual (Alternativa)

Se o comando `nodes ssh` nÃ£o funcionar:

```bash
# 1. Pegar IP do node
kubernetes-create nodes list

# 2. Pegar chave SSH (no primeiro deploy)
# A chave estÃ¡ em ~/.ssh/ ou no output do Pulumi

# 3. SSH manual
ssh -i ~/.ssh/cluster-key root@<node-ip>
```

---

## â¬†ï¸ 5. Upgrade do Kubernetes

### Upgrade para VersÃ£o EspecÃ­fica

```bash
# Upgrade para versÃ£o especÃ­fica
kubernetes-create nodes upgrade --version v1.29.0+rke2r1
```

### Upgrade para Latest Stable

```bash
# Upgrade para Ãºltima versÃ£o estÃ¡vel
kubernetes-create nodes upgrade
```

### Preview do Upgrade (Dry-Run)

```bash
# Ver o que serÃ¡ feito sem executar
kubernetes-create nodes upgrade --version v1.29.0+rke2r1 --dry-run
```

### Processo de Upgrade

```
â¬†ï¸  Upgrade Kubernetes

ğŸ“‹ Upgrade Plan:
  â€¢ Current Version: v1.28.5+rke2r1
  â€¢ Target Version: v1.29.0+rke2r1
  â€¢ Upgrade Order:
    1. Master nodes (one by one)
    2. Worker nodes (rolling update)

âš ï¸  IMPORTANT:
  â€¢ Backup your cluster before upgrading
  â€¢ Test in staging environment first
  â€¢ Expect brief downtime during master upgrades

Do you want to proceed with the upgrade? (y/N): y

ğŸ”„ Upgrading master-1...
âœ… master-1 upgraded successfully

ğŸ”„ Upgrading master-2...
âœ… master-2 upgraded successfully

ğŸ”„ Upgrading master-3...
âœ… master-3 upgraded successfully

ğŸ”„ Upgrading workers (rolling)...
âœ… All workers upgraded successfully

âœ… Cluster upgraded to v1.29.0+rke2r1!
```

### Workflow Recomendado

```bash
# 1. BACKUP PRIMEIRO!
kubernetes-create backup create

# 2. Testar em staging
kubernetes-create nodes upgrade \
  --version v1.29.0+rke2r1 \
  --stack staging

# 3. Verificar staging estÃ¡ OK
kubernetes-create nodes list --stack staging
kubectl get nodes --stack staging

# 4. Fazer upgrade em produÃ§Ã£o
kubernetes-create nodes upgrade \
  --version v1.29.0+rke2r1 \
  --stack production

# 5. Verificar produÃ§Ã£o
kubernetes-create nodes list
kubectl get nodes
```

### Workflow via Config (Alternativo)

```bash
# 1. Editar cluster.yaml
vim cluster.yaml
# kubernetes:
#   version: v1.29.0+rke2r1  # Atualizar versÃ£o

# 2. Preview
kubernetes-create deploy --config cluster.yaml --dry-run

# 3. Aplicar
kubernetes-create deploy --config cluster.yaml

# 4. Verificar
kubernetes-create nodes list
```

### âš ï¸ PrecauÃ§Ãµes

1. **Backup**: SEMPRE faÃ§a backup antes de upgrade
2. **Staging**: Teste em staging primeiro
3. **DocumentaÃ§Ã£o**: Leia release notes da nova versÃ£o
4. **Compatibilidade**: Verifique compatibilidade de addons
5. **Rollback Plan**: Tenha plano de rollback pronto
6. **Maintenance Window**: FaÃ§a em horÃ¡rio de baixo uso
7. **Monitoring**: Monitore durante e apÃ³s upgrade

### VersÃµes Suportadas

```bash
# RKE2 Stable
v1.28.x+rke2r1
v1.29.x+rke2r1
v1.30.x+rke2r1

# Verificar versÃµes disponÃ­veis
curl -s https://update.rke2.io/v1-release/channels | jq
```

---

## ğŸ“Š Casos de Uso Comuns

### 1. Escalar Workers para Atender Demanda

```bash
# SituaÃ§Ã£o: AplicaÃ§Ã£o precisa de mais recursos

# Ver uso atual
kubectl top nodes

# Adicionar 2 workers
vim cluster.yaml  # Aumentar count de workers
kubernetes-create deploy --config cluster.yaml

# Verificar
kubernetes-create nodes list
kubectl get nodes
```

### 2. Remover Node Com Problema

```bash
# SituaÃ§Ã£o: Node com problema de hardware

# Identificar node problemÃ¡tico
kubernetes-create nodes list
# worker-2 estÃ¡ com status NotReady

# Remover node problemÃ¡tico
kubernetes-create nodes remove worker-2

# Node serÃ¡ recriado automaticamente no prÃ³ximo deploy
```

### 3. Maintenance de um Node

```bash
# SituaÃ§Ã£o: Precisa fazer manutenÃ§Ã£o no node

# 1. SSH no node
kubernetes-create nodes ssh worker-1

# 2. Fazer manutenÃ§Ã£o
# ... comandos de manutenÃ§Ã£o ...

# 3. Sair
exit

# 4. Verificar se voltou ao normal
kubernetes-create nodes list
```

### 4. Investigar Problema

```bash
# SituaÃ§Ã£o: Pods falhando em um node especÃ­fico

# 1. SSH no node
kubernetes-create nodes ssh worker-1

# 2. Ver logs
journalctl -u rke2-agent -f

# 3. Ver containers
crictl ps
crictl logs <container-id>

# 4. Ver recursos
top
df -h

# 5. Se necessÃ¡rio, remover e recriar
exit
kubernetes-create nodes remove worker-1
kubernetes-create deploy --config cluster.yaml
```

### 5. Upgrade Seguro

```bash
# SituaÃ§Ã£o: Nova versÃ£o do Kubernetes disponÃ­vel

# 1. Backup
kubernetes-create backup create

# 2. Testar em staging
kubernetes-create nodes upgrade \
  --version v1.29.0+rke2r1 \
  --stack staging

# 3. Verificar staging
kubernetes-create nodes list --stack staging
# Rodar testes da aplicaÃ§Ã£o

# 4. Se OK, fazer em produÃ§Ã£o
kubernetes-create nodes upgrade \
  --version v1.29.0+rke2r1 \
  --stack production

# 5. Monitorar
watch -n 5 'kubernetes-create nodes list'
```

---

## ğŸ¯ Boas PrÃ¡ticas

### 1. Always Backup Before Changes

```bash
# Antes de qualquer operaÃ§Ã£o destrutiva
kubernetes-create backup create
```

### 2. Use Dry-Run

```bash
# Sempre faÃ§a preview primeiro
kubernetes-create deploy --config cluster.yaml --dry-run
```

### 3. Maintain HA

```bash
# Masters sempre em nÃºmero Ã­mpar
# MÃ­nimo 3 para HA
nodePools:
  - name: masters
    count: 3  # âœ… Ãmpar
    # count: 4  # âŒ Par
```

### 4. Label Your Nodes

```yaml
# No cluster.yaml
nodePools:
  - name: workers
    labels:
      workload: web
      environment: production
```

### 5. Monitor After Changes

```bash
# ApÃ³s adicionar/remover/upgrade
watch -n 5 'kubernetes-create nodes list'
kubectl get nodes -w
kubectl get pods --all-namespaces -w
```

---

## ğŸš¨ Troubleshooting

### Node NÃ£o Aparece

```bash
# Verificar se foi criado no provider
# DigitalOcean: https://cloud.digitalocean.com/droplets
# Linode: https://cloud.linode.com/linodes

# Ver logs do Pulumi
kubernetes-create deploy --config cluster.yaml --verbose
```

### Node em NotReady

```bash
# SSH no node
kubernetes-create nodes ssh <node-name>

# Ver logs
journalctl -u rke2-server -f  # Master
journalctl -u rke2-agent -f   # Worker

# Ver se RKE2 estÃ¡ rodando
systemctl status rke2-server
systemctl status rke2-agent

# Reiniciar se necessÃ¡rio
systemctl restart rke2-server
systemctl restart rke2-agent
```

### SSH NÃ£o Funciona

```bash
# 1. Verificar IP
kubernetes-create nodes list

# 2. Testar conectividade
ping <node-ip>

# 3. Verificar firewall
# Permitir SSH (porta 22) no firewall do provider

# 4. SSH manual
ssh -i ~/.ssh/cluster-key root@<node-ip>
```

### Upgrade Falhou

```bash
# 1. Verificar status
kubernetes-create nodes list

# 2. Ver logs
kubernetes-create nodes ssh master-1
journalctl -u rke2-server -f

# 3. Rollback se necessÃ¡rio
# Editar cluster.yaml com versÃ£o anterior
kubernetes-create deploy --config cluster.yaml

# 4. Restaurar backup se crÃ­tico
kubernetes-create backup restore <backup-id>
```

---

## ğŸ“š Resumo dos Comandos

```bash
# Listar
kubernetes-create nodes list
kubernetes-create nodes list --stack production

# Adicionar
kubernetes-create nodes add --count 2
kubernetes-create nodes add --count 3 --size s-4vcpu-8gb
kubernetes-create nodes add --provider linode --region us-east

# Remover
kubernetes-create nodes remove worker-3
kubernetes-create nodes remove worker-3 --force

# SSH
kubernetes-create nodes ssh master-1
kubernetes-create nodes ssh worker-2

# Upgrade
kubernetes-create nodes upgrade
kubernetes-create nodes upgrade --version v1.29.0+rke2r1
kubernetes-create nodes upgrade --dry-run
```

---

## ğŸ‰ ConclusÃ£o

O gerenciamento de nodes Ã© essencial para manter um cluster saudÃ¡vel e escalÃ¡vel. Use estes comandos para:

âœ… **Monitorar** - Ver status de todos os nodes
âœ… **Escalar** - Adicionar/remover conforme necessidade
âœ… **Debugar** - SSH para investigar problemas
âœ… **Atualizar** - Manter Kubernetes atualizado

**Sempre lembre:**
- ğŸ”’ Backup antes de mudanÃ§as
- ğŸ‘€ Dry-run para preview
- ğŸ“Š Monitor apÃ³s operaÃ§Ãµes
- ğŸ§ª Teste em staging primeiro

Happy Node Management! ğŸš€
